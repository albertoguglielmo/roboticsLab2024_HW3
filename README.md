# ğŸš€ **Homework 3: Vision-Based Control** ğŸŒğŸ¤– 

<p align="center">
  <img src="image.png" alt="Homework 3 Banner" width="80%">
</p>


Welcome to Homework 3! The primary goal of this Homework is to implement a vision-based controller for a 7-DOF robotic manipulator!  The goal is to use ROS2 packages to detect objects, implement vision-based control, and perform complex trajectories using both joint space and Cartesian space. Below you'll find step-by-step instructions and commands to run various tasks in the Gazebo simulation environment.

---

## ğŸŸ¢ **FIRST POINT: Creating a Gazebo World** ğŸ› ï¸

The first step is to create a custom Gazebo world named **`empty_new.world`** containing a blue spherical object. This object will be detected by a camera mounted on the robotâ€™s end-effector and processed using OpenCV for blob detection.

### 1.a â¡ï¸ Launch the Gazebo World
```bash
ros2 launch iiwa_bringup iiwa.launch_new.py command_interface:="velocity" robot_controller:="velocity_controller" use_sim:=true use_vision:=true
```
### 1.b â¡ï¸ Spherical Object View ğŸŒ

Once the world is launched, the robotâ€™s camera is checked to ensure it can "see" the blue spherical object. The real-time image is visualized using the rqt_image_view tool.
```bash
ros2 launch iiwa_bringup iiwa.launch_new.py command_interface:="velocity" robot_controller:="velocity_controller" use_sim:=true use_vision:=true

ros2 run rqt_image_view rqt_image_view
```
### 1.c â¡ï¸ Blob Detection ğŸ”µ

Using OpenCV, the node ros2_opencv_node detects the blob of the blue spherical object in the scene. This demonstrates the systemâ€™s ability to process images for object detection tasks.
```bash
ros2 launch iiwa_bringup iiwa.launch_new.py command_interface:="velocity" robot_controller:="velocity_controller" use_sim:=true use_vision:=true

ros2 run rqt_image_view rqt_image_view

ros2 run ros2_opencv ros2_opencv_node
```

---

## ğŸ”µ **SECOND POINT: Vision-Based Controller** ğŸ•¹ï¸

The second point focuses on implementing a vision-based controller to align the robotâ€™s camera with an ArUco marker. It also explores advanced control using both joint space and Cartesian space.

### 2.a â¡ï¸ Task: Positioning with ArUco Marker ğŸ“¸

This section spawns a world with an ArUco marker. The robot aligns its camera with the marker by adjusting its position and orientation based on vision feedback. 

```bash
ros2 launch iiwa_bringup iiwa.launch.py command_interface:="velocity" robot_controller:="velocity_controller" use_sim:=true use_vision:=true

ros2 launch aruco_ros single.launch.py marker_size:=0.1 marker_id:=201

ros2 run rqt_image_view rqt_image_view

ros2 run ros2_kdl_package ros2_kdl_vision_control --ros-args -p cmd_interface:=velocity -p task:=positioning
```
ğŸ“¹ Watch the demo on YouTube : https://www.youtube.com/watch?v=dgOmxvEkXrY&feature=youtu.be

### 2.b â¡ï¸ Task: Look at Point with ArUco MarkerğŸ“Œ 

In this task, the camera aligns with a specific point in space (e.g., the ArUco marker) by computing the appropriate control commands.

```bash
ros2 run ros2_kdl_package ros2_kdl_vision_control --ros-args -p task:=look-at-point
```
ğŸ“¹ Demo here : https://www.youtube.com/watch?v=EEsBwHIai5A

## ğŸ’ª Effort-Based Control ğŸ› ï¸
This section extends the control capabilities by using the "**effort**" interface for more precise torque-based control.

### 2.c â¡ï¸ Task: Look-at-point with Effort Controller ğŸ’ª
Launch the simulation:
```bash
ros2 launch iiwa_bringup iiwa.launch.py command_interface:="effort" robot_controller:="effort_controller" use_sim:=true use_vision:=true

ros2 launch aruco_ros single.launch.py marker_size:=0.1 marker_id:=201

ros2 run ros2_kdl_package ros2_kdl_vision_control --ros-args -p cmd_interface:=effort -p task:=look-at-point
```
Replace the effort values to switch to Cartesian torque computation as needed.

ğŸ“¹ Watch the YouTube demo : https://www.youtube.com/watch?v=TxIn_ssuXvQ

### 2.d  â¡ï¸ Task: Linear Position Trajectory ğŸ¯ + Look-at-point Vision ğŸ”

The robot combines a linear position **trajectory** with the **look-at-point** task.The robot simultaneously tracks a trajectory in space while maintaining visual alignment with an ArUco marker.

Launch the simulation:
```bash
ros2 launch iiwa_bringup iiwa.launch.py command_interface:="effort" robot_controller:="effort_controller" use_sim:=true use_vision:=true

ros2 launch aruco_ros single.launch.py marker_size:=0.1 marker_id:=201

ros2 run ros2_kdl_package ros2_kdl_vision_control --ros-args -p cmd_interface:=effort -p task:=trajectory_lap 
```
> **Note:**  
> The robot will be launched with the **Joint Space controller**.  
---
ğŸ“¹ Check out the video : https://www.youtube.com/watch?v=b3cn5bcMcGs
> **Note:**
> You can go from the Joint Space controller to the **Cartesian Space** controller by changing the code.
